{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'cifar10'\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Coursera\\\\ProjectX'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_dir = os.path.join(data_dir, 'train_x_' + str(1) + '.npy')\n",
    "train_y_dir = os.path.join(data_dir, 'train_y_' + str(1) + '.npy')\n",
    "\n",
    "train_x = np.load(train_x_dir)\n",
    "train_y = np.load(train_y_dir)\n",
    "\n",
    "test_x_dir = os.path.join(data_dir, 'test_x_6.npy')\n",
    "test_y_dir = os.path.join(data_dir, 'test_y_6.npy')\n",
    "\n",
    "test_x = np.load(test_x_dir)\n",
    "test_y = np.load(test_y_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm trying to implement below model structure from the paper:\n",
    "### http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title]([cifar10 project] CNN model structure from paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure from paper:\n",
    "### Conv_1 -> Max_pool_1 -> Conv_2 -> Max_pool_2 -> Conv_3 -> Conv_4 -> Conv_5 -> Max_pool_3\n",
    "### -> Fully_conn_1 -> Fully_conn_2 -> Fully_conn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "evaluation_size = 500\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "target_size = len(labels)\n",
    "num_channels = 3 \n",
    "generations = 1000\n",
    "eval_every = 5\n",
    "conv1_features = 100\n",
    "conv2_features = 50\n",
    "conv3_features = 50\n",
    "conv4_features = 50\n",
    "conv5_features = 50\n",
    "\n",
    "\n",
    "max_pool_size1 = 2 \n",
    "max_pool_size2 = 2 \n",
    "max_pool_size3 = 2 \n",
    "\n",
    "fully_connected_size1 = 200\n",
    "fully_connected_size2 = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_width = 28\n",
    "crop_height = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate prediction accuracy \n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis=1)\n",
    "    num_correct = np.sum(np.equal(batch_predictions, targets))\n",
    "    return 100. * num_correct/batch_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIRJFwS9Eoij4hUiU/wUxu48HEAmy+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18bcbca35f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(train_x[2])#, cmap='Greys_r')\n",
    "plt.show()\n",
    "print(labels[train_y[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_build(input_data, conv1_weight, conv1_bias, conv2_weight, conv2_bias, conv3_weight, conv3_bias, conv4_weight, conv4_bias,conv5_weight, conv5_bias, full1_weight, full1_bias, full2_weight, full2_bias, full3_weight, full3_bias):\n",
    "\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(max_pool2, conv3_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "    \n",
    "    conv4 = tf.nn.conv2d(relu3, conv4_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_bias))\n",
    "    \n",
    "    \n",
    "    conv5 = tf.nn.conv2d(relu4, conv5_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "    max_pool3 = tf.nn.max_pool(relu5, ksize=[1, max_pool_size3, max_pool_size3, 1],\n",
    "                               strides=[1, max_pool_size3, max_pool_size3, 1], padding='SAME')\n",
    "    \n",
    "    \n",
    "    final_conv_shape = max_pool3.get_shape().as_list()\n",
    "    modified_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool3, [final_conv_shape[0], modified_shape])\n",
    "\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    \n",
    "    fully_connected2 = tf.nn.relu(tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias))\n",
    "    \n",
    "    model_output = tf.add(tf.matmul(fully_connected2, full3_weight), full3_bias)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_run(train_data, train_labels, test_data, test_labels, generations):\n",
    "    x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "    x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "    y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    eval_input_shape = (evaluation_size, image_width, image_height, num_channels)\n",
    "    eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "    eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n",
    "    \n",
    "    conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "    conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n",
    "    conv3_weight = tf.Variable(tf.truncated_normal([4, 4, conv2_features, conv3_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv3_bias = tf.Variable(tf.zeros([conv3_features], dtype=tf.float32))\n",
    "    conv4_weight = tf.Variable(tf.truncated_normal([4, 4, conv3_features, conv4_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv4_bias = tf.Variable(tf.zeros([conv4_features], dtype=tf.float32))\n",
    "    conv5_weight = tf.Variable(tf.truncated_normal([4, 4, conv4_features, conv5_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv5_bias = tf.Variable(tf.zeros([conv5_features], dtype=tf.float32))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    resulting_width = image_width // (max_pool_size1 * max_pool_size2 * max_pool_size3)\n",
    "    resulting_height = image_height // (max_pool_size1 * max_pool_size2 * max_pool_size3)\n",
    "    full1_input_size = resulting_width * resulting_height * conv5_features\n",
    "    full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, fully_connected_size2], stddev=0.1, dtype=tf.float32))\n",
    "    full2_bias = tf.Variable(tf.truncated_normal([fully_connected_size2], stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    \n",
    "    full3_weight = tf.Variable(tf.truncated_normal([fully_connected_size2, target_size], stddev=0.1, dtype=tf.float32))\n",
    "    full3_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    model_output = model_build(x_input, conv1_weight, conv1_bias, conv2_weight, conv2_bias, conv3_weight, conv3_bias, conv4_weight, conv4_bias, conv5_weight, conv5_bias, full1_weight, full1_bias, full2_weight, full2_bias, full3_weight, full3_bias)\n",
    "    test_model_output = model_build(eval_input, conv1_weight, conv1_bias, conv2_weight, conv2_bias, conv3_weight, conv3_bias, conv4_weight, conv4_bias, conv5_weight, conv5_bias, full1_weight, full1_bias, full2_weight, full2_bias, full3_weight, full3_bias)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "    \n",
    "    my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize Variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print('model running')\n",
    "    \n",
    "    prediction = tf.nn.softmax(model_output)\n",
    "    test_prediction = tf.nn.softmax(test_model_output)\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(generations):\n",
    "        rand_index = np.random.choice(len(train_data), size=batch_size)\n",
    "        rand_x = train_data[rand_index]\n",
    "        #rand_x = np.expand_dims(rand_x, 3)\n",
    "        rand_y = train_labels[rand_index]\n",
    "        train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "        \n",
    "        temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "        temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "\n",
    "        if (i+1) % eval_every == 0:\n",
    "            eval_index = np.random.choice(len(test_data), size=evaluation_size)\n",
    "            eval_x = test_data[eval_index]\n",
    "            #eval_x = np.expand_dims(eval_x, 3)\n",
    "            eval_y = test_labels[eval_index]\n",
    "            test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "            test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "            temp_test_acc = get_accuracy(test_preds, eval_y)\n",
    "\n",
    "            # Record and print results\n",
    "            train_loss.append(temp_train_loss)\n",
    "            train_acc.append(temp_train_acc)\n",
    "            test_acc.append(temp_test_acc)\n",
    "            acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "            acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        if (i+1) % eval_every  == 0:\n",
    "            print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "    return train_loss, train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running\n",
      "Generation # 5. Train Loss: 245.72. Train Acc (Test Acc): 7.00 (9.00)\n",
      "Generation # 10. Train Loss: 11.90. Train Acc (Test Acc): 13.00 (9.80)\n",
      "Generation # 15. Train Loss: 8.27. Train Acc (Test Acc): 15.00 (11.00)\n",
      "Generation # 20. Train Loss: 4.23. Train Acc (Test Acc): 6.00 (11.60)\n",
      "Generation # 25. Train Loss: 3.09. Train Acc (Test Acc): 8.00 (10.60)\n",
      "Generation # 30. Train Loss: 2.52. Train Acc (Test Acc): 11.00 (10.40)\n",
      "Generation # 35. Train Loss: 2.39. Train Acc (Test Acc): 15.00 (9.60)\n",
      "Generation # 40. Train Loss: 2.34. Train Acc (Test Acc): 11.00 (11.60)\n",
      "Generation # 45. Train Loss: 2.32. Train Acc (Test Acc): 14.00 (10.80)\n",
      "Generation # 50. Train Loss: 2.39. Train Acc (Test Acc): 13.00 (10.80)\n",
      "Generation # 55. Train Loss: 2.36. Train Acc (Test Acc): 13.00 (13.20)\n",
      "Generation # 60. Train Loss: 2.25. Train Acc (Test Acc): 11.00 (10.60)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-3c9e60963f9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-129-9b1ec66d7a99>\u001b[0m in \u001b[0;36mmodel_run\u001b[1;34m(train_data, train_labels, test_data, test_labels, generations)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mrand_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrand_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrand_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtemp_train_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_train_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00005\n",
    "batch_size = 100\n",
    "\n",
    "train_loss, train_acc, test_acc = model_run(train_x, train_y, test_x, test_y, generations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_image = tf.image.resize_image_with_crop_or_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.image.per_image_standardization(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'per_image_standardization:0' shape=(32, 32, 3) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_build_simple(input_data, conv1_weight, conv1_bias, full1_weight, full1_bias, full2_weight, full2_bias):\n",
    "    #print(input_data)\n",
    "    #print(conv1_weight)\n",
    "\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "    norm1 = tf.nn.lrn(max_pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75)\n",
    "\n",
    "    \n",
    "    final_conv_shape = norm1.get_shape().as_list()\n",
    "    modified_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool1, [final_conv_shape[0], modified_shape])\n",
    "\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    \n",
    "    model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_run_simple(train_data, train_labels, test_data, test_labels, generations):\n",
    "    x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "    \n",
    "    x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "    \n",
    "    y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    eval_input_shape = (evaluation_size, image_width, image_height, num_channels)\n",
    "    eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "    eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n",
    "    \n",
    "    conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "    conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    resulting_width = image_width // (max_pool_size1)\n",
    "    resulting_height = image_height // (max_pool_size1)\n",
    "    full1_input_size = resulting_width * resulting_height * conv1_features\n",
    "    full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "    full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    model_output = model_build_simple(x_input, conv1_weight, conv1_bias, full1_weight, full1_bias, full2_weight, full2_bias)\n",
    "    test_model_output = model_build_simple(eval_input, conv1_weight, conv1_bias, full1_weight, full1_bias, full2_weight, full2_bias)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "    \n",
    "    my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize Variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print('model running')\n",
    "    \n",
    "    prediction = tf.nn.softmax(model_output)\n",
    "    test_prediction = tf.nn.softmax(test_model_output)\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for i in range(generations):\n",
    "        rand_index = np.random.choice(len(train_data), size=batch_size)\n",
    "        rand_x = train_data[rand_index]\n",
    "        #rand_x = np.expand_dims(rand_x, 3)\n",
    "        rand_y = train_labels[rand_index]\n",
    "        train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "        temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "        temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "\n",
    "        if (i+1) % eval_every == 0:\n",
    "            #eval_index = np.random.choice(len(test_data), size=evaluation_size)\n",
    "            #eval_x = test_data[eval_index]\n",
    "            #eval_y = test_labels[eval_index]\n",
    "            #test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "            #test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "            temp_test_acc = 0 # get_accuracy(test_preds, eval_y)\n",
    "\n",
    "            # Record and print results\n",
    "            train_loss.append(temp_train_loss)\n",
    "            train_acc.append(temp_train_acc)\n",
    "            test_acc.append(temp_test_acc)\n",
    "            acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "            acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        if (i+1) % (eval_every ) == 0:\n",
    "            print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "    return train_loss, train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-ead95dc8f45e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_run_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-169-7d230d058d43>\u001b[0m in \u001b[0;36mmodel_run_simple\u001b[1;34m(train_data, train_labels, test_data, test_labels, generations)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrand_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrand_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mtemp_train_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_train_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mtemp_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_train_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "\n",
    "train_loss, train_acc, test_acc = model_run_simple(train_x, train_y, test_x, test_y, generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_1_build(input_data, full1_weight, full1_bias, full2_weight, full2_bias):\n",
    "    # Transform data to 1*N for fully connected NN\n",
    "    input_data_shape = input_data.get_shape().as_list()\n",
    "    modified_shape = input_data_shape[1] * input_data_shape[2] * input_data_shape[3] \n",
    "    flat_data = tf.reshape(input_data, [input_data_shape[0], modified_shape])\n",
    "    print(flat_data)\n",
    "    print(full1_weight)\n",
    "    # Fully Connected Layer 1\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_data, full1_weight), full1_bias))\n",
    "\n",
    "    # Fully Connected Layer 2\n",
    "    model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_1(train_data, train_labels, test_data, test_labels, generations):\n",
    "    # set up placeholders\n",
    "    x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "    x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "    y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    eval_input_shape = (evaluation_size, image_width, image_height, num_channels)\n",
    "    eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "    eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n",
    "    \n",
    "    # set up variables\n",
    "    resulting_width = image_width\n",
    "    resulting_height = image_height\n",
    "    full1_input_size = resulting_width * resulting_height * num_channels\n",
    "    full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "    full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "    full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    # outputs and loss\n",
    "    model_output = model_1_build(x_input, full1_weight, full1_bias, full2_weight, full2_bias)\n",
    "    test_model_output = model_1_build(eval_input, full1_weight, full1_bias, full2_weight, full2_bias)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "    \n",
    "    my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize Variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print('model 1 (full_conn_1 -> full_connect_2 -> predictions)')\n",
    "    \n",
    "    prediction = tf.nn.softmax(model_output)\n",
    "    test_prediction = tf.nn.softmax(test_model_output)\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for i in range(generations):\n",
    "        rand_index = np.random.choice(len(train_data), size=batch_size)\n",
    "        rand_x = train_data[rand_index]\n",
    "        #rand_x = np.expand_dims(rand_x, 3)\n",
    "        rand_y = train_labels[rand_index]\n",
    "        train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "        \n",
    "        temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "        temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "\n",
    "        if (i+1) % eval_every == 0:\n",
    "            eval_index = np.random.choice(len(test_data), size=evaluation_size)\n",
    "            eval_x = test_data[eval_index]\n",
    "            #eval_x = np.expand_dims(eval_x, 3)\n",
    "            eval_y = test_labels[eval_index]\n",
    "            test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "            test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "            temp_test_acc = get_accuracy(test_preds, eval_y)\n",
    "\n",
    "            # Record and print results\n",
    "            train_loss.append(temp_train_loss)\n",
    "            train_acc.append(temp_train_acc)\n",
    "            test_acc.append(temp_test_acc)\n",
    "            acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "            acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "            print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "    return train_loss, train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_69:0\", shape=(500, 3072), dtype=float32)\n",
      "<tf.Variable 'Variable_396:0' shape=(3072, 200) dtype=float32_ref>\n",
      "Tensor(\"Reshape_70:0\", shape=(500, 3072), dtype=float32)\n",
      "<tf.Variable 'Variable_396:0' shape=(3072, 200) dtype=float32_ref>\n",
      "model 1 (full_conn_1 -> full_connect_2 -> predictions)\n",
      "Generation # 5. Train Loss: 2.31. Train Acc (Test Acc): 9.80 (9.80)\n",
      "Generation # 10. Train Loss: 2.34. Train Acc (Test Acc): 8.20 (9.80)\n",
      "Generation # 15. Train Loss: 2.32. Train Acc (Test Acc): 9.80 (10.60)\n",
      "Generation # 20. Train Loss: 2.32. Train Acc (Test Acc): 11.40 (9.80)\n",
      "Generation # 25. Train Loss: 2.31. Train Acc (Test Acc): 9.60 (9.40)\n",
      "Generation # 30. Train Loss: 2.31. Train Acc (Test Acc): 9.20 (8.40)\n",
      "Generation # 35. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (9.60)\n",
      "Generation # 40. Train Loss: 2.30. Train Acc (Test Acc): 6.60 (10.60)\n",
      "Generation # 45. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (11.40)\n",
      "Generation # 50. Train Loss: 2.30. Train Acc (Test Acc): 9.40 (9.40)\n",
      "Generation # 55. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (10.60)\n",
      "Generation # 60. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (10.60)\n",
      "Generation # 65. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (8.40)\n",
      "Generation # 70. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (8.60)\n",
      "Generation # 75. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (10.80)\n",
      "Generation # 80. Train Loss: 2.30. Train Acc (Test Acc): 13.00 (9.40)\n",
      "Generation # 85. Train Loss: 2.30. Train Acc (Test Acc): 13.60 (8.40)\n",
      "Generation # 90. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (7.60)\n",
      "Generation # 95. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (10.80)\n",
      "Generation # 100. Train Loss: 2.30. Train Acc (Test Acc): 12.40 (9.40)\n",
      "Generation # 105. Train Loss: 2.30. Train Acc (Test Acc): 12.00 (9.40)\n",
      "Generation # 110. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (9.00)\n",
      "Generation # 115. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (10.40)\n",
      "Generation # 120. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (10.00)\n",
      "Generation # 125. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (10.60)\n",
      "Generation # 130. Train Loss: 2.30. Train Acc (Test Acc): 9.40 (9.40)\n",
      "Generation # 135. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (8.80)\n",
      "Generation # 140. Train Loss: 2.30. Train Acc (Test Acc): 12.00 (9.00)\n",
      "Generation # 145. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (10.60)\n",
      "Generation # 150. Train Loss: 2.30. Train Acc (Test Acc): 8.60 (8.00)\n",
      "Generation # 155. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (10.20)\n",
      "Generation # 160. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (10.20)\n",
      "Generation # 165. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (9.20)\n",
      "Generation # 170. Train Loss: 2.30. Train Acc (Test Acc): 8.00 (11.40)\n",
      "Generation # 175. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (11.00)\n",
      "Generation # 180. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (9.40)\n",
      "Generation # 185. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (8.00)\n",
      "Generation # 190. Train Loss: 2.30. Train Acc (Test Acc): 8.20 (8.20)\n",
      "Generation # 195. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (9.00)\n",
      "Generation # 200. Train Loss: 2.30. Train Acc (Test Acc): 12.60 (7.60)\n",
      "Generation # 205. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (10.60)\n",
      "Generation # 210. Train Loss: 2.31. Train Acc (Test Acc): 10.40 (7.80)\n",
      "Generation # 215. Train Loss: 2.31. Train Acc (Test Acc): 8.40 (8.80)\n",
      "Generation # 220. Train Loss: 2.29. Train Acc (Test Acc): 10.60 (10.00)\n",
      "Generation # 225. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (8.20)\n",
      "Generation # 230. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (12.40)\n",
      "Generation # 235. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (7.80)\n",
      "Generation # 240. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (11.60)\n",
      "Generation # 245. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (10.00)\n",
      "Generation # 250. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (10.60)\n",
      "Generation # 255. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (8.20)\n",
      "Generation # 260. Train Loss: 2.30. Train Acc (Test Acc): 7.40 (6.20)\n",
      "Generation # 265. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (8.40)\n",
      "Generation # 270. Train Loss: 2.30. Train Acc (Test Acc): 12.40 (11.20)\n",
      "Generation # 275. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (11.20)\n",
      "Generation # 280. Train Loss: 2.31. Train Acc (Test Acc): 10.40 (8.80)\n",
      "Generation # 285. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (10.80)\n",
      "Generation # 290. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (9.40)\n",
      "Generation # 295. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (7.80)\n",
      "Generation # 300. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (10.40)\n",
      "Generation # 305. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (12.40)\n",
      "Generation # 310. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (10.00)\n",
      "Generation # 315. Train Loss: 2.30. Train Acc (Test Acc): 8.60 (10.60)\n",
      "Generation # 320. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (11.80)\n",
      "Generation # 325. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (11.00)\n",
      "Generation # 330. Train Loss: 2.30. Train Acc (Test Acc): 12.00 (9.00)\n",
      "Generation # 335. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (7.80)\n",
      "Generation # 340. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (9.40)\n",
      "Generation # 345. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (10.40)\n",
      "Generation # 350. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (9.60)\n",
      "Generation # 355. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (8.00)\n",
      "Generation # 360. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (7.40)\n",
      "Generation # 365. Train Loss: 2.31. Train Acc (Test Acc): 8.80 (10.40)\n",
      "Generation # 370. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (9.20)\n",
      "Generation # 375. Train Loss: 2.30. Train Acc (Test Acc): 7.40 (9.20)\n",
      "Generation # 380. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (9.60)\n",
      "Generation # 385. Train Loss: 2.30. Train Acc (Test Acc): 12.20 (10.20)\n",
      "Generation # 390. Train Loss: 2.31. Train Acc (Test Acc): 9.60 (10.00)\n",
      "Generation # 395. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (11.40)\n",
      "Generation # 400. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (12.40)\n",
      "Generation # 405. Train Loss: 2.30. Train Acc (Test Acc): 12.00 (8.20)\n",
      "Generation # 410. Train Loss: 2.30. Train Acc (Test Acc): 7.40 (9.40)\n",
      "Generation # 415. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (10.80)\n",
      "Generation # 420. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (11.40)\n",
      "Generation # 425. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (10.20)\n",
      "Generation # 430. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (9.20)\n",
      "Generation # 435. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (9.20)\n",
      "Generation # 440. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (9.20)\n",
      "Generation # 445. Train Loss: 2.31. Train Acc (Test Acc): 8.60 (10.60)\n",
      "Generation # 450. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (9.00)\n",
      "Generation # 455. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (8.00)\n",
      "Generation # 460. Train Loss: 2.30. Train Acc (Test Acc): 9.40 (9.60)\n",
      "Generation # 465. Train Loss: 2.30. Train Acc (Test Acc): 12.60 (10.00)\n",
      "Generation # 470. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (8.40)\n",
      "Generation # 475. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (12.20)\n",
      "Generation # 480. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (8.60)\n",
      "Generation # 485. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (10.00)\n",
      "Generation # 490. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (9.00)\n",
      "Generation # 495. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (9.00)\n",
      "Generation # 500. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (10.00)\n",
      "Generation # 505. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (11.20)\n",
      "Generation # 510. Train Loss: 2.30. Train Acc (Test Acc): 12.00 (11.80)\n",
      "Generation # 515. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (7.60)\n",
      "Generation # 520. Train Loss: 2.30. Train Acc (Test Acc): 12.20 (8.80)\n",
      "Generation # 525. Train Loss: 2.31. Train Acc (Test Acc): 9.20 (9.80)\n",
      "Generation # 530. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (9.40)\n",
      "Generation # 535. Train Loss: 2.30. Train Acc (Test Acc): 14.80 (11.60)\n",
      "Generation # 540. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (8.20)\n",
      "Generation # 545. Train Loss: 2.31. Train Acc (Test Acc): 8.40 (11.00)\n",
      "Generation # 550. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (7.60)\n",
      "Generation # 555. Train Loss: 2.30. Train Acc (Test Acc): 12.80 (11.80)\n",
      "Generation # 560. Train Loss: 2.31. Train Acc (Test Acc): 9.40 (11.80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 565. Train Loss: 2.31. Train Acc (Test Acc): 8.80 (9.40)\n",
      "Generation # 570. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (13.00)\n",
      "Generation # 575. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (9.20)\n",
      "Generation # 580. Train Loss: 2.30. Train Acc (Test Acc): 7.40 (12.20)\n",
      "Generation # 585. Train Loss: 2.31. Train Acc (Test Acc): 10.20 (9.80)\n",
      "Generation # 590. Train Loss: 2.30. Train Acc (Test Acc): 12.60 (11.20)\n",
      "Generation # 595. Train Loss: 2.30. Train Acc (Test Acc): 13.40 (10.60)\n",
      "Generation # 600. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (9.20)\n",
      "Generation # 605. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (9.60)\n",
      "Generation # 610. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (9.80)\n",
      "Generation # 615. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (8.20)\n",
      "Generation # 620. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (12.40)\n",
      "Generation # 625. Train Loss: 2.30. Train Acc (Test Acc): 6.80 (10.20)\n",
      "Generation # 630. Train Loss: 2.31. Train Acc (Test Acc): 8.00 (9.40)\n",
      "Generation # 635. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (11.80)\n",
      "Generation # 640. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (11.00)\n",
      "Generation # 645. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (12.00)\n",
      "Generation # 650. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (8.20)\n",
      "Generation # 655. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (13.00)\n",
      "Generation # 660. Train Loss: 2.31. Train Acc (Test Acc): 7.80 (9.00)\n",
      "Generation # 665. Train Loss: 2.30. Train Acc (Test Acc): 13.60 (7.80)\n",
      "Generation # 670. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (9.80)\n",
      "Generation # 675. Train Loss: 2.30. Train Acc (Test Acc): 12.20 (9.40)\n",
      "Generation # 680. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (9.20)\n",
      "Generation # 685. Train Loss: 2.31. Train Acc (Test Acc): 8.00 (10.00)\n",
      "Generation # 690. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (11.20)\n",
      "Generation # 695. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (10.20)\n",
      "Generation # 700. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (8.40)\n",
      "Generation # 705. Train Loss: 2.30. Train Acc (Test Acc): 12.20 (8.40)\n",
      "Generation # 710. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (12.20)\n",
      "Generation # 715. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (10.20)\n",
      "Generation # 720. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (9.40)\n",
      "Generation # 725. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (8.20)\n",
      "Generation # 730. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (7.60)\n",
      "Generation # 735. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (10.00)\n",
      "Generation # 740. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (7.80)\n",
      "Generation # 745. Train Loss: 2.31. Train Acc (Test Acc): 7.60 (9.00)\n",
      "Generation # 750. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (8.80)\n",
      "Generation # 755. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (9.00)\n",
      "Generation # 760. Train Loss: 2.30. Train Acc (Test Acc): 13.60 (10.60)\n",
      "Generation # 765. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (9.60)\n",
      "Generation # 770. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (8.20)\n",
      "Generation # 775. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (10.20)\n",
      "Generation # 780. Train Loss: 2.30. Train Acc (Test Acc): 11.20 (9.40)\n",
      "Generation # 785. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (12.60)\n",
      "Generation # 790. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (9.80)\n",
      "Generation # 795. Train Loss: 2.30. Train Acc (Test Acc): 12.60 (10.80)\n",
      "Generation # 800. Train Loss: 2.30. Train Acc (Test Acc): 10.40 (10.00)\n",
      "Generation # 805. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (8.40)\n",
      "Generation # 810. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (11.60)\n",
      "Generation # 815. Train Loss: 2.30. Train Acc (Test Acc): 11.40 (11.00)\n",
      "Generation # 820. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (10.80)\n",
      "Generation # 825. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (10.80)\n",
      "Generation # 830. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (9.60)\n",
      "Generation # 835. Train Loss: 2.31. Train Acc (Test Acc): 10.20 (10.00)\n",
      "Generation # 840. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (8.60)\n",
      "Generation # 845. Train Loss: 2.30. Train Acc (Test Acc): 10.00 (8.80)\n",
      "Generation # 850. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (10.00)\n",
      "Generation # 855. Train Loss: 2.30. Train Acc (Test Acc): 9.60 (10.20)\n",
      "Generation # 860. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (9.00)\n",
      "Generation # 865. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (9.80)\n",
      "Generation # 870. Train Loss: 2.30. Train Acc (Test Acc): 12.20 (9.00)\n",
      "Generation # 875. Train Loss: 2.30. Train Acc (Test Acc): 9.40 (10.40)\n",
      "Generation # 880. Train Loss: 2.30. Train Acc (Test Acc): 10.20 (10.80)\n",
      "Generation # 885. Train Loss: 2.30. Train Acc (Test Acc): 9.00 (8.60)\n",
      "Generation # 890. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (13.80)\n",
      "Generation # 895. Train Loss: 2.30. Train Acc (Test Acc): 12.60 (9.40)\n",
      "Generation # 900. Train Loss: 2.30. Train Acc (Test Acc): 8.80 (12.20)\n",
      "Generation # 905. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (9.60)\n",
      "Generation # 910. Train Loss: 2.30. Train Acc (Test Acc): 12.80 (11.20)\n",
      "Generation # 915. Train Loss: 2.30. Train Acc (Test Acc): 13.20 (9.60)\n",
      "Generation # 920. Train Loss: 2.31. Train Acc (Test Acc): 9.00 (12.60)\n",
      "Generation # 925. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (13.00)\n",
      "Generation # 930. Train Loss: 2.31. Train Acc (Test Acc): 9.20 (8.00)\n",
      "Generation # 935. Train Loss: 2.31. Train Acc (Test Acc): 7.40 (10.80)\n",
      "Generation # 940. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (9.20)\n",
      "Generation # 945. Train Loss: 2.31. Train Acc (Test Acc): 5.80 (10.80)\n",
      "Generation # 950. Train Loss: 2.30. Train Acc (Test Acc): 9.20 (10.60)\n",
      "Generation # 955. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (11.80)\n",
      "Generation # 960. Train Loss: 2.30. Train Acc (Test Acc): 11.00 (9.80)\n",
      "Generation # 965. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (10.20)\n",
      "Generation # 970. Train Loss: 2.30. Train Acc (Test Acc): 10.80 (10.00)\n",
      "Generation # 975. Train Loss: 2.30. Train Acc (Test Acc): 13.20 (10.60)\n",
      "Generation # 980. Train Loss: 2.30. Train Acc (Test Acc): 8.40 (11.40)\n",
      "Generation # 985. Train Loss: 2.30. Train Acc (Test Acc): 10.60 (7.60)\n",
      "Generation # 990. Train Loss: 2.30. Train Acc (Test Acc): 11.60 (9.60)\n",
      "Generation # 995. Train Loss: 2.30. Train Acc (Test Acc): 11.80 (10.00)\n",
      "Generation # 1000. Train Loss: 2.30. Train Acc (Test Acc): 9.80 (11.80)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 500\n",
    "generations = 1000\n",
    "train_loss_1, train_acc_1, test_acc_1 = model_1(train_x, train_y, test_x, test_y, generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
